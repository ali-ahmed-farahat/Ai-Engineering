Testing and evaluation has so many methods, we explained only 3:

1-Train and Test evaluation:
  -choose data without labels and train and test on the same dataset 
  -which leads to high Training accuracy and low out-of-sample accuracy

*NOTES:
  Error = 1/n Σ |y- y(hat)|
  Training accuracy: accuracy of the model's output in the data it's trained on, which is not always a good thing as this may cause 
                     overfitting to the trained dataset
  Out-of-sample accuracy: accuracy of the model on data on which the model wasn't trained on 
                          and it's the most important as it serves the main purpose of the model

2-Train/Test split evaluation:
  -if the dataset has 10 rows then 5 for testing and 5 for training 
  -more accurate evaluation on the out-of-sample accuracy
  -dependent on the dataset

3-K-fold cross evaluation:
  -if the data set has 12 rows then it will go as rounds (in the 1st the first 3 rows are for testing and the other 9 for testing and so on)
  -then the accuracy is calculated by the average of the accuracy of each round (sum of average/number of rounds)

Accuracy Metrics For Evaluation: the difference between data points and the trendline generated by the algorithm

  -MAE ( Mean Absolute Error): 1/n Σ |yi-y(hat)i|
  -MSE ( Mean Square Error): 1/n Σ (yi -yhati)2
  -RMSE ( Root Mean Square Error) most popular: √"1/n Σ (yi -yhati)2"
  -RAE ( Relative Absolute Error): Σ |yi -yhati|  /   Σ |yi -y bar|
  -RSE ( Relative Square Error): Σ (yi -yhati)2  /   Σ (yi -y bar)2
  -R2 = 1- RSE ( the higher R squared is, the better the model fits your data
